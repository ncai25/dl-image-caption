{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5OHaGch1U6m"
      },
      "source": [
        "# HW5: Image Captioning\n",
        "---\n",
        "\n",
        "This is the Notebook that goes with **Homework 5: Image Captioning**! \n",
        "\n",
        "In this notebook, you can run the assignments main method and train either the RNN or the Transformer model, instead of running the assignment on your personal machine. In addition, you can visualize the self-attention layer in your TransformerDecoder, and generate captions using both of your models for images in the test dataset. \n",
        "\n",
        "This notebook can be ported to Colab very quickly, so please feel free to try that out! It might also make some of the training quicker..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1gOuhXVXYQo"
      },
      "source": [
        "## Preparation Code\n",
        "\n",
        "If need be, feel free to pull your content from GitHub using this or a similar cell of choice. This should be moderately standard-practice for some of you. Additionally, remember to %cd to the required directories as needed for your workflow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AhGwcFfE7OnO"
      },
      "outputs": [],
      "source": [
        "# #@title Github Clone A Repository\n",
        "# #@markdown **NOTE**: Must use access token as password. To make one, go [here](https://github.com/settings/tokens) and save your token!\n",
        "\n",
        "# from IPython.display import clear_output\n",
        "# import sys, os\n",
        "\n",
        "# git_user_default = 'your-github-username'\n",
        "# git_user = 'your-github-username'  #@param {type:\"string\"}\n",
        "# if git_user == git_user_default:\n",
        "#   git_user = input(\"Enter your Github username: \")\n",
        "# parent_repo = 'Brown-Deep-Learning'\n",
        "# assignment_name = 'homework5_imagecaption'  #@param {type:\"string\"}\n",
        "# github_repo = f'{parent_repo}/{assignment_name}-{git_user}'\n",
        "# force_reclone = True           #@param {type:\"boolean\"}\n",
        "# is_private_repo = True            #@param {type:\"boolean\"}\n",
        "# keep_gh_login = True            #@param {type:\"boolean\"}\n",
        "\n",
        "# user_dir, github_dir = github_repo.split('/')\n",
        "# data_file = f'{github_dir}/hw5/data/data.p'\n",
        "\n",
        "# if not os.path.isdir(github_dir) or force_reclone:\n",
        "    \n",
        "#     if is_private_repo:\n",
        "#         if 'git_user' not in globals() or 'git_pass' not in globals():\n",
        "#             # git_user = input(\"Username: \")\n",
        "#             git_pass = input(\"GH Token: \")\n",
        "#             clear_output() \n",
        "\n",
        "#     if force_reclone:\n",
        "#         !rm -rf {github_dir} &> /dev/null\n",
        "\n",
        "#     if is_private_repo:\n",
        "#         !git clone https://{git_user}:{git_pass}@github.com/{github_repo}.git\n",
        "#         if not keep_gh_login:\n",
        "#             del git_user, git_pass \n",
        "#     else: \n",
        "#         !git clone https://github.com/{github_repo}.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvuKoBRGYAwf"
      },
      "source": [
        "This block of code imports the classes you completed in your assignment, along with additional libraries needed for the visualizations.\n",
        "\n",
        "Feel free to add autoimport queries as needed. This notebook's code will not be auto-ran by the autograder (only the outputs will be looked at during manual grading), so do what you need to here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ARiGr47j7T-I"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from model import ImageCaptionModel\n",
        "from decoder import TransformerDecoder, RNNDecoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This assignment uses the Flickr 8k dataset! Let's go ahead and pull that in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_captions:   (35455, 21)\n",
            "test_captions:    (5000, 21)\n",
            "\n",
            "train_img_feats:  (35455, 2048)\n",
            "test_img_feats:   (5000, 2048)\n",
            "\n",
            "train_images:     (500, 224, 224, 3)\n",
            "test_images:      (500, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "## Before this, download the dataset and run preprocessing.py as instructed. \n",
        "## This may take like 10 mins, but should only happen once so ok.\n",
        "## https://www.kaggle.com/datasets/adityajn105/flickr8k?resource=download\n",
        "\n",
        "with open('../data/data.p', 'rb') as data_file:\n",
        "    data_dict = pickle.load(data_file)\n",
        "\n",
        "# As mentioned in the handout, this assignment has 5 captions per image. This block of code \n",
        "# expands the image_feature lists to have 5 copies of each image to correspond to each of their captions \n",
        "feat_prep = lambda x: np.repeat(np.array(x).reshape(-1, 2048), 5, axis=0)\n",
        "img_prep  = lambda x: np.repeat(x, 5, axis=0)\n",
        "\n",
        "## Captions; preprocessed sentences with 20 window size\n",
        "train_captions  = np.array(data_dict['train_captions']);            print('train_captions:  ', train_captions.shape)\n",
        "test_captions   = np.array(data_dict['test_captions']);             print('test_captions:   ', test_captions.shape)\n",
        "\n",
        "## 2048-D resnet embeddings of images.\n",
        "train_img_feats = feat_prep(data_dict['train_image_features']);     print('\\ntrain_img_feats: ', train_img_feats.shape)\n",
        "test_img_feats  = feat_prep(data_dict['test_image_features']);      print('test_img_feats:  ', test_img_feats.shape)\n",
        "\n",
        "## Small subset of actual images for visualization purposes. \n",
        "## These are just for the first 100 images of each (clones 5 times)\n",
        "train_images    = img_prep(data_dict['train_images']);              print('\\ntrain_images:    ', train_images.shape)\n",
        "test_images     = img_prep(data_dict['test_images']);               print('test_images:     ', test_images.shape)\n",
        "\n",
        "## Conversion dictionaries to go between word and label index\n",
        "word2idx        = data_dict['word2idx']\n",
        "idx2word        = data_dict['idx2word']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the images take up a lot of data, we only kept a sliver of the original images. Feel free to update the preprocessing to retain all of the images if you'd like. Below is a visualization of some of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        print(f'Caption {j+1}:', ' '.join([idx2word[idx] for idx in train_captions[i * 5 + j]]))\n",
        "    plt.imshow(train_images[i * 5])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO0tgcqO77g1"
      },
      "source": [
        "## Training your model\n",
        "\n",
        "As always you can complete and run this assignments main method on your personal machine. However, you can also choose to run the assignment in this notebook to take advantage of Colab's GPU allocation! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iIKs4lo0iIo"
      },
      "source": [
        "### Running your RNN model\n",
        "\n",
        "Depending on your use cases, you may choose to structure your model in a variety of ways. In contrast to previous assignments, this one is intended to mimic a lot of modern research-oriented repositories you might find in the wild. Specifically: **Instead of providing easy-to-use APIs for experimenters, they rigidify their implementation to make tests replicable.** Specifically, they may provide a command-line interface and define testing/training procedures which log results. \n",
        "\n",
        "(I mean, ideally you can make a flexible API and allow for both ease of extension and examples to demonstrate how your results were gathered, but sometimes researchers only have so much time...)\n",
        "\n",
        "Once you have filled in the `model.py` components and the `RNNDecoder` of the `decoder.py` file, run this block to train your RNN model. As you can see, the hyperparamets default to the ones you use in `assignment.py`'s argparse specification, but feel free to change any of them to try to improve your model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can investigate `assignment.py` to find that main will try to parse command-line arguments and fill in a variety of defaults. Specifically, you'll find this: \n",
        "```python\n",
        "def parse_args(args=None):\n",
        "    \"\"\" \n",
        "    Perform command-line argument parsing (other otherwise parse arguments with defaults). \n",
        "    To parse in an interative context (i.e. in notebook), add required arguments.\n",
        "    These will go into args and will generate a list that can be passed in.\n",
        "    For example: \n",
        "        parse_args('--type', 'rnn', ...)\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(...)\n",
        "    parser.add_argument('--type',           required=True,              ...)\n",
        "    parser.add_argument('--task',           required=True,              ...)\n",
        "    parser.add_argument('--data',           required=True,              ...')\n",
        "    parser.add_argument('--epochs',         type=int,   default=3,      ...)\n",
        "    parser.add_argument('--lr',             type=float, default=1e-3,   ...)\n",
        "    parser.add_argument('--optimizer',      type=str,   default='adam', ...)\n",
        "    parser.add_argument('--batch_size',     type=int,   default=100,    ...)\n",
        "    parser.add_argument('--hidden_size',    type=int,   default=256,    ...)\n",
        "    parser.add_argument('--window_size',    type=int,   default=20,     ...)\n",
        "    parser.add_argument('--chkpt_path',     default='',                 ...)\n",
        "    parser.add_argument('--check_valid',    default=True,               ...)\n",
        "    if args is None: \n",
        "        return parser.parse_args()      ## For calling through command line\n",
        "    return parser.parse_args(args)      ## For calling through notebook.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When trying to run the file by default, you'll get a nice usage error message if you are missing any required arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: assignment.py [-h] --type {rnn,transformer} --task {train,test,both}\n",
            "                     --data DATA [--epochs EPOCHS] [--lr LR]\n",
            "                     [--optimizer {adam,rmsprop,sgd}]\n",
            "                     [--batch_size BATCH_SIZE] [--hidden_size HIDDEN_SIZE]\n",
            "                     [--window_size WINDOW_SIZE] [--chkpt_path CHKPT_PATH]\n",
            "                     [--check_valid]\n",
            "assignment.py: error: the following arguments are required: --type, --task, --data\n"
          ]
        }
      ],
      "source": [
        "!python assignment.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows what kinds of arguments can be passed into your python file via main, and is reminiscent of what you might have seen in HW3. \n",
        "\n",
        "The following command will therefore be sufficient to try what an author (or you) might consider to be a \"default training run\" of the model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IxtDkQEx8gcg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid 50/50]\t loss=3.017\t acc: 0.320\t perp: 20.43436\n",
            "[Valid 50/50]\t loss=2.845\t acc: 0.337\t perp: 17.20615\n",
            "[Valid 50/50]\t loss=2.794\t acc: 0.343\t perp: 16.34843\n",
            "Model saved to ../rnn_model\n"
          ]
        }
      ],
      "source": [
        "## TODO: Increase epochs to a larger size when ready (maybe 2 or 3 would be enough?)\n",
        "!python assignment.py --type rnn --task train --data ../data/data.p --epochs 3 --chkpt_path ../rnn_model\n",
        "## if using colab, you may need to do something like the following or might need to %cd into the directory of interest first..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this command also saves the model, we should be able to load it back in and use it. Feel free to modify the saving utility as needed based on your modifications, but the default system should work fine for the initial requirements. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model loaded from '../rnn_model'\n",
            "[Valid 50/50]\t loss=2.794\t acc: 0.343\t perp: 16.348\n"
          ]
        }
      ],
      "source": [
        "!python assignment.py --type rnn --task test --data ../data/data.p --chkpt_path ../rnn_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvWXYsyQ0iIp"
      },
      "source": [
        "### Running your Transformer model\n",
        "\n",
        "Once you have completed the `transformer.py` file, run this block to train your transformer based model. Note that running with the `both` task will both train, save, and test your model in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xDXL5-huovlE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/noracai/Documents/CS1470/homework-5p-image-captioning-norafk/code/assignment.py\", line 171, in <module>\n",
            "    main(parse_args())\n",
            "  File \"/Users/noracai/Documents/CS1470/homework-5p-image-captioning-norafk/code/assignment.py\", line 76, in main\n",
            "    train_model(\n",
            "  File \"/Users/noracai/Documents/CS1470/homework-5p-image-captioning-norafk/code/assignment.py\", line 149, in train_model\n",
            "    stats += [model.train(captions, img_feats, pad_idx, batch_size=args.batch_size)]\n",
            "  File \"/Users/noracai/Documents/CS1470/homework-5p-image-captioning-norafk/code/model.py\", line 56, in train\n",
            "    loss = self.loss_function(probs, decoder_labels, mask) # losses.append(loss)\n",
            "  File \"/Users/noracai/Documents/CS1470/homework-5p-image-captioning-norafk/code/model.py\", line 167, in loss_function\n",
            "    masked_prbs = tf.boolean_mask(prbs, mask)\n",
            "  File \"/Users/noracai/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/Users/noracai/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 103, in convert_to_eager_tensor\n",
            "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
            "ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n"
          ]
        }
      ],
      "source": [
        "## TODO: Increase epochs to a larger size when ready (maybe 2 or 3 would be enough?)\n",
        "!python assignment.py --type transformer --task both --data ../data/data.p --epochs 4 --lr 0.0005 --chkpt_path ../transform_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should be able to reach validation perplexity in the ballpark of 15-18 by the end of training! We found that around 4 epochs was enough for our settings, but your results may vary. Though you are not constrained by any time limits, know when to stop and try to be proactive with your time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_bkKyU0iIq"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "After training our Transformer model, you can visualize the self-attention layer to examine the behavior of your attention heads and see if any patterns emerge. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpz85avktKxt"
      },
      "source": [
        "To test out the components of the model interactively, you'll need to deconstruct selections of the model/runner code and get an instance of the model in an interactive context (aka inside the notebook). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "No file or directory found at ../transform_model",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m args \u001b[38;5;241m=\u001b[39m parse_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--type rnn --task both --data ../data/data.p\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     13\u001b[0m args\u001b[38;5;241m.\u001b[39mchkpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../transform_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m tra_imcap \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m args\u001b[38;5;241m.\u001b[39mchkpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../rnn_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m rnn_imcap \u001b[38;5;241m=\u001b[39m load_model(args)\n",
            "File \u001b[0;32m~/Documents/CS1470/homework-5p-image-captioning-norafk/code/assignment.py:108\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(args):\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Loads model by reference based on arguments. Also returns said model'''\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchkpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAttentionHead\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAttentionHead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAttentionMatrix\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAttentionMatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMultiHeadedAttention\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiHeadedAttention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mTransformerBlock\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformerBlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mPositionalEncoding\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPositionalEncoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mTransformerDecoder\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTransformerDecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mRNNDecoder\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRNNDecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mImageCaptionModel\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mImageCaptionModel\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m## Saving is very nuanced. Might need to set the custom components correctly.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m## Functools.partial is a function wrapper that auto-fills a selection of arguments. \u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m## so in other words, the first argument of ImageCaptionModel.test is model (for self)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at ../transform_model"
          ]
        }
      ],
      "source": [
        "## Feel free to insert auto-reloads as necessary\n",
        "from assignment import parse_args, load_model\n",
        "from decoder import TransformerDecoder, RNNDecoder\n",
        "\n",
        "## Pull your model into the notebook. This is heavily based off of assignment.py, \n",
        "## and feel free to reuse as much as you want. Your final project will probably \n",
        "## involve a lot of this investigative reverse-engineering based on what repos \n",
        "## you have to stumble upon.\n",
        "## You're not in a notebook scenario, so use get_default_arguments and feel free to update it...\n",
        "\n",
        "args = parse_args('--type rnn --task both --data ../data/data.p'.split())\n",
        "\n",
        "args.chkpt_path = '../transform_model'\n",
        "tra_imcap = load_model(args)\n",
        "\n",
        "args.chkpt_path = '../rnn_model'\n",
        "rnn_imcap = load_model(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn_imcap.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tra_imcap.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our model, we need to be able to actually access the attention matrix that gets generated by out model. So that we can visualize it, right? Unfortunately for us, some convenience methods that allow you to make arbitrary model slices (i.e. the Functional API) are forfeit since our model is a subclass (in contrast to a sequential or functional). \n",
        "\n",
        "However, we can still dig into the model and force out way to computing the components we want. Our weights have been saved, after all..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yChcaClyump_"
      },
      "source": [
        "The following block of code visualizes the decoder self-attention for a random images in the test dataset. \n",
        "\n",
        "\n",
        "Move your mouse over the words in the left hand column, and see how much attention your decoder self-attention layer pays to each word in the sentance as it encodes each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y4cKlpWvwUf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from vis_utils import plot_decoder_text_attention\n",
        "import numpy as np\n",
        "\n",
        "index = np.random.choice(np.array(list(range(0,500,5))))\n",
        "\n",
        "caption    = test_captions[index][:-1]\n",
        "image_feat = test_img_feats[index]\n",
        "image      = test_images[index]\n",
        "\n",
        "print(\"Image number:\", index)\n",
        "\n",
        "def get_attention(tra_imcap, image_feat, caption):\n",
        "    ## TODO: If you're implementing multi-headed attension, you may need to change \n",
        "    ## some stuff to display to display all of the attention matrices.\n",
        "\n",
        "    ## Into impac decoder (NOTE: expand_dims only necessary for Transformer)\n",
        "    encoded_images = tra_imcap.decoder.image_embedding(tf.expand_dims(image_feat, 1))\n",
        "    # captions = tra_imcap.decoder.embedding(caption)\n",
        "    captions = tra_imcap.decoder.encoding(caption)\n",
        "    ## Into imcap TransformerBlock; get self-attention\n",
        "    AttentionHead = tra_imcap.decoder.decoder.self_atten\n",
        "    K = tf.tensordot(captions, AttentionHead.K, 1)\n",
        "    V = tf.tensordot(captions, AttentionHead.V, 1)\n",
        "    self_atten = AttentionHead.attn_mtx((K, V))\n",
        "    ## Into imcap TransformerBlock; get context self-attention\n",
        "    AttentionHead = tra_imcap.decoder.decoder.self_context_atten\n",
        "    K = tf.tensordot(captions, AttentionHead.K, 1)\n",
        "    V = tf.tensordot(captions, AttentionHead.V, 1)\n",
        "    self_context_atten = AttentionHead.attn_mtx((K, V))\n",
        "    return self_atten, self_context_atten\n",
        "\n",
        "\n",
        "def vis_attention(atten_mtx, image_features, caption, idx2word):\n",
        "    caption_words = [idx2word[idx] for idx in caption]\n",
        "    end_sentance_index = caption_words.index('<end>') if '<end>' in caption_words else 20\n",
        "    caption_words = caption_words[:end_sentance_index]\n",
        "    AttentionMatrix = atten_mtx[:, :end_sentance_index, :end_sentance_index]\n",
        "    AttentionMatrix = tf.reshape(AttentionMatrix, (1, 1, 1, end_sentance_index, end_sentance_index))\n",
        "    plot_decoder_text_attention(attention=AttentionMatrix, tokens=caption_words)\n",
        "\n",
        "self_atten, self_context_atten = get_attention(\n",
        "    tra_imcap, tf.expand_dims(image_feat, 0), tf.expand_dims(caption, 0)\n",
        ")\n",
        "\n",
        "print(\"self_atten\")\n",
        "vis_attention(self_atten, image_feat, caption, idx2word)\n",
        "\n",
        "print(\"self_context_atten\")\n",
        "vis_attention(self_context_atten, image_feat, caption, idx2word)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTYAclU7ALb"
      },
      "source": [
        "### Caption Generation\n",
        "Now that you have trained both of your models, it's time to use them to generate original captions for images in the testing set. First, the model is given the <start\\> token and asked to generate probabilites for the next word in the sequence. The next token is chosen by sampling from that probability. This process repeats until the model generates the <end\\> token, or the maximum sequence length is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zar8W0Zn7Lnr"
      },
      "source": [
        " \n",
        "\n",
        "\n",
        "\n",
        "There is still one piece of this equation missing. The tokens are sampled from the probabilities your models generate, but your models were required to output logits, not probabilities. This is becasue this assignment, like many NLP models, uses temperature as a parameter in text generation. If the models sampled from  probabilies calculated by simply applying softmax to the logits, then the probability of the most likely word will usually be very high and the models will usually genrate the same, most probable caption every time. We use the temperature as a parameter to even out the probabilites so the model produces more 'creative' captions. This is done by dividing the logits by the temperature parameter before applying softmax. Higher temprature values will give a more creative captiong, while temprature values closer to 0 will be more greedy. Check out [this](https://lukesalamone.github.io/posts/what-is-temperature/) article for a demonstration and further explaination of temprature in NLP models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNzx9-qP_Th"
      },
      "source": [
        "The following blocks of code will generate a caption for the image currently selected for the attention visualization above. Try playing around with different temperature values and see how it changes the captions your models generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_DpuFiYMOIa"
      },
      "outputs": [],
      "source": [
        "def gen_caption_temperature(model, image_embedding, wordToIds, padID, temp, window_length):\n",
        "    \"\"\"\n",
        "    Function used to generate a caption using an ImageCaptionModel given\n",
        "    an image embedding. \n",
        "    \"\"\"\n",
        "    idsToWords = {id: word for word, id in wordToIds.items()}\n",
        "    unk_token = wordToIds['<unk>']\n",
        "    caption_so_far = [wordToIds['<start>']]\n",
        "    while len(caption_so_far) < window_length and caption_so_far[-1] != wordToIds['<end>']:\n",
        "        caption_input = np.array([caption_so_far + ((window_length - len(caption_so_far)) * [padID])])\n",
        "        logits = model(np.expand_dims(image_embedding, 0), caption_input)\n",
        "        logits = logits[0][len(caption_so_far) - 1]\n",
        "        probs = tf.nn.softmax(logits / temp).numpy()\n",
        "        next_token = unk_token\n",
        "        attempts = 0\n",
        "        while next_token == unk_token and attempts < 5:\n",
        "            next_token = np.random.choice(len(probs), p=probs)\n",
        "            attempts += 1\n",
        "        caption_so_far.append(next_token)\n",
        "    return ' '.join([idsToWords[x] for x in caption_so_far][1:-1])\n",
        "\n",
        "temperature = .05\n",
        "gen_caption_temperature(tra_imcap, image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NNogJ5TL_-v"
      },
      "outputs": [],
      "source": [
        "temperature = 0.2\n",
        "gen_caption_temperature(tra_imcap, image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**NOTE:** You may want to try a different image. Sometimes you get really unlucky with random selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Sentences for Training Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperature = 0.05\n",
        "indices = np.random.choice(np.array(list(range(0, 500, 5))), 10, replace=False)\n",
        "for i in indices:\n",
        "    curr_image_feat = train_img_feats[i]\n",
        "    curr_image      = train_images[i]\n",
        "    for j in range(5):  ## Display all of the captions trained on\n",
        "        words = [idx2word[x] for x in train_captions[i+j][:-1] if idx2word[x] not in ('<pad>', '<start>', '<end>')]\n",
        "        print(f'C{j+1}:', ' '.join(words))\n",
        "    print('RNN:', gen_caption_temperature(rnn_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
        "    print('TRA:', gen_caption_temperature(tra_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
        "    plt.imshow(curr_image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying out on things in testing set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C1: biker is racing <unk>\n",
            "C2: biker <unk> to go down hill\n",
            "C3: person in <unk> suit rides bike down hill\n",
            "C4: person in blue <unk> uniform is <unk> dirt bike on grassy track\n",
            "C5: person riding bicycle <unk> down an <unk> hill\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'gen_caption_temperature' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     words \u001b[38;5;241m=\u001b[39m [idx2word[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m test_captions[i\u001b[38;5;241m+\u001b[39mj][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx2word[x] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words))\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mgen_caption_temperature\u001b[49m(rnn_imcap, curr_image_feat, word2idx, word2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m], temperature, args\u001b[38;5;241m.\u001b[39mwindow_size))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRA:\u001b[39m\u001b[38;5;124m'\u001b[39m, gen_caption_temperature(tra_imcap, curr_image_feat, word2idx, word2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m], temperature, args\u001b[38;5;241m.\u001b[39mwindow_size))\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(curr_image)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gen_caption_temperature' is not defined"
          ]
        }
      ],
      "source": [
        "temperature = 0.05\n",
        "indices = np.random.choice(np.array(list(range(0, 500, 5))), 10, replace=False)\n",
        "for i in indices:\n",
        "    curr_image_feat = test_img_feats[i]\n",
        "    curr_image      = test_images[i]\n",
        "    for j in range(5):  ## Display all of the captions trained on\n",
        "        words = [idx2word[x] for x in test_captions[i+j][:-1] if idx2word[x] not in ('<pad>', '<start>', '<end>')]\n",
        "        print(f'C{j+1}:', ' '.join(words))\n",
        "    print('RNN:', gen_caption_temperature(rnn_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
        "    print('TRA:', gen_caption_temperature(tra_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
        "    plt.imshow(curr_image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpwoQazfkfv1"
      },
      "source": [
        "# Conclusion!\n",
        "Congrats! You have finished this assignment! Below, put down your favorite captions that your RNN and Transformer models both generated!  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n-1aiQFliGn"
      },
      "outputs": [],
      "source": [
        "## TODO: fill in the ? and display the vis images with the generated caption below it\n",
        "\n",
        "rnn_image_index = 42#?\n",
        "rnn_caption = gen_caption_temperature(rnn_imcap, test_img_feats[rnn_image_index], word2idx, word2idx['<pad>'], temperature, args.window_size)\n",
        "\n",
        "tra_image_index = 23#?\n",
        "tra_caption = gen_caption_temperature(tra_imcap, test_img_feats[tra_image_index], word2idx, word2idx['<pad>'], temperature, args.window_size)\n",
        "\n",
        "print(rnn_caption)\n",
        "plt.imshow(test_images[rnn_image_index])\n",
        "plt.show()\n",
        "\n",
        "print(tra_caption)\n",
        "plt.imshow(test_images[tra_image_index])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "im_cap_notebook.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('dl3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "895b6b526044b58cdb0796603b6137eb4df401700b6bb2bfb9582034a97581c4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
